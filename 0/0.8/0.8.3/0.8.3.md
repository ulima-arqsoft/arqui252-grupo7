> [0. Acerca del Grupo](../../0.md) ‚Ä∫ [0.8. Temas Individuales (Parte 2)](../0.8.md) ‚Ä∫ [0.8.3. Integrante 3](0.8.3.md)

# 0.8.3. Integrante 3 ‚Äì Evaluaci√≥n Comparativa de Modelos LLM en Tareas de Programaci√≥n

**Nombre:** Luis Mario Martinez Rueda  
**Curso:** Arquitectura de Software  
**Docente:** Jose Caballero Ortiz   
**Tema:** Benchmarking de LLMs para Generaci√≥n de C√≥digo  

---

## 1. Problema

Los modelos de lenguaje (LLMs) se utilizan cada vez m√°s para **asistir en la generaci√≥n autom√°tica de c√≥digo**, tanto en contextos educativos como profesionales.  
Sin embargo, evaluar su rendimiento es complejo debido a:

- Diferencias en calidad del c√≥digo generado.  
- Variaciones en tiempos de respuesta (latencia).  
- Resultados inconsistentes en ejecuciones repetidas.  
- Falta de un est√°ndar unificado para comparar modelos.  

Para tomar decisiones informadas sobre qu√© modelo utilizar, se requiere un **benchmark controlado**, que mida objetivamente la capacidad real de los LLMs para resolver ejercicios de programaci√≥n.

---

## 2. Enfoque de Soluci√≥n ‚Äì Benchmarking de LLMs

El objetivo del proyecto es crear un **proceso de evaluaci√≥n manual** para comparar LLMs en la generaci√≥n de c√≥digo Python, aplicando m√©tricas simples, reproducibles y extra√≠das de literatura acad√©mica.

### Modelos evaluados:

| Modelo | Plataforma | Tipo |
|--------|------------|------|
| **Gemini 3 Pro Preview** | Google AI Studio | LLM general razonador |
| **Claude 4.5 Sonnet** | Anthropic | LLM optimizado para tareas complejas |

### Dataset utilizado:

Se utiliz√≥ un subconjunto de 5 problemas del dataset **MBPP (Mostly Basic Python Problems)**, un dataset ampliamente referenciado en papers de benchmarking.

Cada problema incluye:

- Enunciado  
- C√≥digo base o firma de funci√≥n  
- Casos de prueba  
- Output esperado  

---

## 3. M√©tricas Seleccionadas

Basadas en estudios acad√©micos sobre evaluaci√≥n de LLMs para generaci√≥n de c√≥digo (Scopus, IEEE, ACL), se emplearon tres m√©tricas:

| M√©trica | Descripci√≥n |
|---------|-------------|
| **EXEC (0/1)** | Indica si el c√≥digo generado se ejecuta sin errores. |
| **TESTS OK / TESTS TOTALES** | Cantidad de casos de prueba superados. |
| **FUNC (%)** | Ratio de √©xito funcional (tests_ok / tests_totales √ó 100). |
| **LAT (s)** | Tiempo que tard√≥ el modelo en generar su respuesta. |

Estas m√©tricas permiten evaluar **exactitud funcional**, **robustez del c√≥digo** y **eficiencia temporal** del modelo.

---

## 4. Proceso Experimental

El experimento consisti√≥ en un flujo manual estandarizado:

1. Seleccionar un ejercicio del dataset MBPP.  
2. Mostrar al modelo el prompt correspondiente.  
3. Recibir √∫nicamente c√≥digo Python sin explicaciones.  
4. Ejecutar el c√≥digo localmente.  
5. Registrar si ocurre error de ejecuci√≥n.  
6. Ejecutar los 3 casos de prueba del ejercicio.  
7. Registrar m√©tricas.  
8. Repetir con el siguiente modelo.  

Para facilitar la interacci√≥n, se utiliz√≥ un **agente en Antigravity (Google)** que permite enviar prompts y obtener respuestas consistentes.

---

## 5. Arquitectura del Proceso de Benchmarking

```mermaid
C4Container
    title Diagrama de Contenedores ‚Äì Sistema de Benchmarking de LLMs

    Person(user, "Investigador", "Ejecuta las pruebas y recopila resultados")

    Container_Boundary(benchsys, "Sistema de Benchmarking") {
        Container(prompting, "Interfaz LLM", "Antigravity / Google AI Studio / Claude Console", "Env√≠a prompts y recibe c√≥digo Python generado por los modelos.")
        Container(localexec, "Entorno Local", "Python", "Compila, ejecuta y valida el c√≥digo generado contra los test cases.")
        ContainerDb(dataset, "Dataset MBPP", "JSONL", "Almacena los problemas de programaci√≥n y los casos de prueba.")
    }

    Rel(user, prompting, "Env√≠a ejercicio y solicita c√≥digo")
    Rel(prompting, localexec, "Entrega c√≥digo generado por los modelos")
    Rel(localexec, dataset, "Carga enunciado y casos de prueba")
    Rel(localexec, user, "Retorna m√©tricas y resultados funcionales")
```

---

## 6. Tabla Comparativa de Resultados

| Task ID | Ejercicio | Modelo | EXEC | Tests OK | Totales | FUNC (%) | LAT (s) |
|--------|-----------|--------|------|----------|----------|------------|-----------|
| 1 | min_cost | Gemini | 1 | 3 | 3 | 100 | 15.55 |
| 1 | min_cost | Claude | 1 | 3 | 3 | 100 | 10.14 |
| 2 | similar_elements | Gemini | 1 | 3 | 3 | 100 | 14.71 |
| 2 | similar_elements | Claude | 1 | 3 | 3 | 100 | 7.75 |
| 3 | is_not_prime | Gemini | 1 | 3 | 3 | 100 | 20.46 |
| 3 | is_not_prime | Claude | 1 | 3 | 3 | 100 | 7.85 |
| 4 | heap_queue_largest | Gemini | 1 | 3 | 3 | 100 | 14.51 |
| 4 | heap_queue_largest | Claude | 1 | 3 | 3 | 100 | 8.26 |
| 5 | count_ways | Gemini | 1 | 3 | 3 | 100 | 16.53 |
| 5 | count_ways | Claude | 1 | 3 | 3 | 100 | 9.82 |

---

## 7. Interpretaci√≥n de Resultados

### Exactitud funcional
Ambos modelos cumplen el 100% de los casos en los 5 ejercicios.  
Esto indica:

- Alto dominio de patrones cl√°sicos de programaci√≥n.  
- Correcta interpretaci√≥n del problema y los constraints.  
- Capacidad s√≥lida para generar c√≥digo ejecutable.  

### Latencia
Aqu√≠ se encuentra la **√∫nica diferencia pr√°ctica observable**:

- **Claude 3.5 Sonnet** es consistentemente m√°s r√°pido (entre 7‚Äì10 s).  
- **Gemini 3 Pro Preview** tiene latencias mayores (14‚Äì20 s).  

Esto sugiere que Claude est√° m√°s optimizado para *code generation* en su modo actual.

---

## 8. Demo

La demostraci√≥n del proyecto incluye:

- Ejecuci√≥n del proceso completo de evaluaci√≥n.  
- Uso del agente Antigravity para enviar prompts.  
- Verificaci√≥n manual del c√≥digo generado.  
- Visualizaci√≥n de la tabla comparativa.  

---

## 9. Conclusi√≥n

- Ambos modelos producen **c√≥digo funcional, correcto y sin errores** para los 5 ejercicios evaluados.  
- Claude ofrece una **latencia significativamente menor**, lo que lo hace m√°s eficiente en tareas interactivas o escenarios con requerimientos de baja espera.  
- La metodolog√≠a propuesta es **simple, manual y reproducible**, ideal para evaluaciones controladas con recursos limitados.  
- Este benchmark demuestra que los LLMs actuales pueden resolver tareas de programaci√≥n b√°sica con **alta fiabilidad**, abriendo paso a estudios m√°s amplios en problemas complejos.

---

## Video del Proyecto

**El siguiente enlace dirige al video donde se explica el desarrollo e implementaci√≥n del Benchmarking de LLMs para Generaci√≥n de C√≥digo‚Äù.**  
- [https://www.youtube.com/watch?v=T1XbBlN6UqE](https://www.youtube.com/watch?v=T1XbBlN6UqE)


---
[‚¨ÖÔ∏è Anterior](../0.8.2/0.8.2.md) | [üè† Home](../../../README.md) | [Siguiente ‚û°Ô∏è](../0.8.4/0.8.4.md)